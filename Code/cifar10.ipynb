{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237e69a1",
   "metadata": {},
   "source": [
    "# Clasifikasi Menggunakan Dataset CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c0848",
   "metadata": {},
   "source": [
    "[Dataset](https://github.com/YoongiKim/CIFAR-10-images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6cc3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from torchmetrics.classification import Accuracy\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f88f6",
   "metadata": {},
   "source": [
    "# dataset dan dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df4de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "    ])\n",
    "\n",
    "train_set = datasets.ImageFolder(root='CIFAR-10/train', transform=train_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True, num_workers=2)\n",
    "test_set = datasets.ImageFolder(root='CIFAR-10/test', transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32259bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2cat, idxclass = train_set.classes, train_set.class_to_idx\n",
    "label2cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32be5881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 32, 32]), torch.Size([256]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, target = next(iter(train_loader))\n",
    "feature.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149942e",
   "metadata": {},
   "source": [
    "# arsitektur dan config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ff60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_feature, out_feature, padding=1, stride=1,\n",
    "                  activation=\"relu\", pool =True, maxpool=True, kernel_size=3,\n",
    "                  kernel_size_pool=2, pool_stride=2)-> list[nn.Sequential]:\n",
    "    layers = [nn.Conv2d(in_feature, out_feature, kernel_size=kernel_size, padding=padding, stride=stride)]\n",
    "    if activation == \"relu\":\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == \"leakyrelu\":\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    elif activation == \"sigmoid\":\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation == \"tanh\":\n",
    "        layers.append(nn.Tanh())\n",
    "    if pool:\n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=kernel_size_pool, stride=pool_stride))\n",
    "        else:\n",
    "            layers.append(nn.AvgPool2d(kernel_size=kernel_size_pool, stride=pool_stride))\n",
    "    else:\n",
    "        layers.append(nn.Identity())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "def linear_block(in_features, out_features, activation=None, dropout=0.0):\n",
    "    layers = [nn.Linear(in_features, out_features)]\n",
    "    # if batch_norm:\n",
    "    #     layers.append(BatchNorm1d(out_features))\n",
    "    if activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'sigmoid':\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'leakyrelu':\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    elif activation == 'softmax':\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "    elif activation == 'elu':\n",
    "        layers.append(nn.ELU())\n",
    "    elif activation == 'selu':\n",
    "        layers.append(nn.SELU())\n",
    "    elif activation == 'lsoftmax':\n",
    "        layers.append(nn.LogSoftmax(dim=1))\n",
    "    if dropout > 0.0:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c5db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(pl.LightningModule):\n",
    "    def __init__(self, num_classes, dropout=0.0):\n",
    "        super(CNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv_block(3, 8),\n",
    "            conv_block(8, 16),\n",
    "            conv_block(16, 32),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            linear_block(32 * 4 * 4, 128, dropout=dropout, activation='relu'),\n",
    "            linear_block(128, 64, dropout=dropout, activation='relu'),\n",
    "            linear_block(64, len(num_classes)),\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=len(num_classes))\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = self.accuracy(outputs, labels)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = self.accuracy(outputs, labels)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.RAdam(self.parameters(), lr=0.005)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5158a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(label2cat, dropout=0.1).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.RAdam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "352f451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0.0 \n",
    "patience = 5\n",
    "epochs_no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecab793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='cifar10-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    ")\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor_callback],\n",
    "    logger=pl.loggers.TensorBoardLogger(\"tb_logs\", name=\"simple_model_experiment\"), # Untuk visualisasi\n",
    "    accelerator='auto',\n",
    "    log_every_n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e20dc7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:658: Checkpoint directory C:\\Users\\dsapu\\Project\\Python\\repo\\CIFAR-10\\Code\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | conv      | Sequential         | 6.0 K  | train\n",
      "1 | fc        | Sequential         | 74.6 K | train\n",
      "2 | criterion | CrossEntropyLoss   | 0      | train\n",
      "3 | accuracy  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "80.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.6 K    Total params\n",
      "0.322     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memulai pelatihan CNN dengan PyTorch Lightning...\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|███▉      | 78/196 [00:17<00:26,  4.47it/s, v_num=4, train_loss_step=1.560, train_acc_step=0.430, val_loss=1.480, val_acc=0.468, train_loss_epoch=1.630, train_acc_epoch=0.410] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(*args, **kwargs)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28mself\u001b[39m._run(model, ckpt_path=ckpt_path)\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28mself\u001b[39m._run_stage()\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28mself\u001b[39m.advance()\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28mself\u001b[39m.epoch_loop.run(\u001b[38;5;28mself\u001b[39m._data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance(data_fetcher)\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:306\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    305\u001b[39m dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m batch, _, __ = \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:134\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     batch = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__next__\u001b[39m()\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:61\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m out = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[39m, in \u001b[36m_MaxSizeCycle.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     out[i] = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterators[i])\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28mself\u001b[39m._get_data()\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1411\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     success, data = \u001b[38;5;28mself\u001b[39m._try_get_data()\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._data_queue.get(timeout=timeout)\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\multiprocessing\\queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    112\u001b[39m timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\multiprocessing\\connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\multiprocessing\\connection.py:346\u001b[39m, in \u001b[36mPipeConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\multiprocessing\\connection.py:896\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    894\u001b[39m                 timeout = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m     ready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    898\u001b[39m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\multiprocessing\\connection.py:828\u001b[39m, in \u001b[36m_exhaustive_wait\u001b[39m\u001b[34m(handles, timeout)\u001b[39m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m     res = _winapi.WaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res == WAIT_TIMEOUT:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMemulai pelatihan CNN dengan PyTorch Lightning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainer.fit(model, train_loader, test_loader)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPelatihan CNN selesai dengan PyTorch Lightning.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel CNN terbaik disimpan di: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_callback.best_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m call._call_and_handle_interrupt(\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[32m    563\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     exit(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nMemulai pelatihan CNN dengan PyTorch Lightning...\")\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "print(\"\\nPelatihan CNN selesai dengan PyTorch Lightning.\")\n",
    "print(f\"Model CNN terbaik disimpan di: {checkpoint_callback.best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5740f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9268d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f3eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baa1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19520918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmsdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
